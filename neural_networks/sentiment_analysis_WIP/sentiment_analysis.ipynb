{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TL;DR:** Initial full attempt resulted in only ~0.5 accuracy on full sample; clearly something is off. Speculated that this was due to the truncated vocabulary used still being too large, and/or not lemmatising, resulting in an inadequate (noisy and/or lossy) word2vec representation. Haven't finished testing this idea yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.callbacks as callbacks\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import word2vec\n",
    "# from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/workspace/data/sentiment_analysis'\n",
    "CHKPT_DIR = f'{DATA_DIR}/checkpoints'\n",
    "LOG_DIR = f'{DATA_DIR}/logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(f'{DATA_DIR}/training.1600000.processed.noemoticon.csv',\n",
    "                     header=None, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = tweets.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1                             2         3                4  \\\n",
       "0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                                   5  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    800000\n",
       "4    800000\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.rename(columns = { 0: 'positive', 5: 'tweet' }, inplace=True)\n",
    "tweets = tweets[['positive', 'tweet']]\n",
    "tweets['positive'] = (tweets.positive == 4).astype(np.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify number of terms (size of vocabulary) to consider\n",
    "using IDF (inverse document frequency) distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectoriser = TfidfVectorizer()\n",
    "doc_terms = vectoriser.fit_transform(tweets.tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=684358, minmax=(2.250207176415574, 14.59236763164987), mean=14.211100645629408, variance=0.7161283022424955, skewness=-3.981211810930763, kurtosis=22.023923629571634)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.describe(vectoriser.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/custom-miniconda/miniconda/envs/custom_python38/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Density'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(vectoriser.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.26621816,  8.40513073, 13.6760769 , ..., 14.59236763,\n",
       "       13.6760769 , 14.59236763])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectoriser.idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find a threshold at which to exclude terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.59236763164987"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(vectoriser.idf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median is equal to the maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.3396046631545"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(vectoriser.idf_, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.676076899775714"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(vectoriser.idf_, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pctl15 = np.percentile(vectoriser.idf_, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2174626.8219081517"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.exp(14.59236763164987)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "869850.7287632598"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.exp(np.percentile(vectoriser.idf_, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1087313.4109540756"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.exp(pctl15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102552,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectoriser.idf_[vectoriser.idf_ < pctl15].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 15th percentile looks like a fine threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenise (and reassemble) Tweets to include only 200,000 most frequent terms\n",
    "\n",
    "This is clearly clunky, but it's the best I can do with Tokenizer's current interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = Tokenizer(num_words=vectoriser.idf_[vectoriser.idf_ < pctl15].shape[0])\n",
    "tokeniser.fit_on_texts(tweets.tweet)\n",
    "tweets.tweet = tokeniser.sequences_to_texts(tokeniser.texts_to_sequences(tweets.tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_word = r'\\b(\\w+?)\\s+\\1(\\s+\\1)+'\n",
    "repl = r'\\1 \\1'\n",
    "tweets['tweet'] = tweets.tweet.apply(lambda s: re.sub(rep_word, repl, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_char = r'(\\w)\\1(\\1)+'\n",
    "repl = r'\\1\\1'\n",
    "tweets['tweet'] = tweets.tweet.apply(lambda s: re.sub(rep_char, repl, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathological corner case(s)\n",
    "ugh = r'\\s+([^\\s]+?)\\s+\\1(\\s+\\1)+'\n",
    "repl = r' \\1 \\1'\n",
    "tweets['tweet'] = tweets.tweet.apply(lambda s: re.sub(ugh, repl, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv(f'{DATA_DIR}/training.1600000.cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write vocabulary into file compatible with word2vec input format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = f'{DATA_DIR}/tweet_vocab_200000.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(vocab_file, 'w')\n",
    "f.write(tweets.tweet.str.cat(sep=' '))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train using vocabulary file and write results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_vectors_file = f'{DATA_DIR}/tweet_vocab_200000_vectors.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.word2vec(vocab_file, vocab_vectors_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create lists of vectors for Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = word2vec.load(vocab_vectors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(t):\n",
    "    if t in word2vec_model:\n",
    "        return word2vec_model[t]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2vecs(w):\n",
    "    return list(map(lambda t: get_vector(t), w))\n",
    "\n",
    "tweets['vectors'] = tweets.tweet.str.split().apply(lambda words: words2vecs(words))\n",
    "tweets['vectors'] = tweets.vectors.apply(lambda vecs: np.array([v for v in vecs if len(v) > 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad list of vectors for each Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch $f'{DATA_DIR}/tweet_vectors.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXWIDTH = tweets.vectors.apply(lambda vecs: len(vecs)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = np.float64\n",
    "DTYPE_BYTES = 8\n",
    "BATCHSIZE = 1000\n",
    "VECDIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 0\n",
      "writing batch 1\n",
      "writing batch 2\n",
      "writing batch 3\n",
      "writing batch 4\n",
      "writing batch 5\n",
      "writing batch 6\n",
      "writing batch 7\n",
      "writing batch 8\n",
      "writing batch 9\n",
      "writing batch 10\n",
      "writing batch 11\n",
      "writing batch 12\n",
      "writing batch 13\n",
      "writing batch 14\n",
      "writing batch 15\n",
      "writing batch 16\n",
      "writing batch 17\n",
      "writing batch 18\n",
      "writing batch 19\n",
      "writing batch 20\n",
      "writing batch 21\n",
      "writing batch 22\n",
      "writing batch 23\n",
      "writing batch 24\n",
      "writing batch 25\n",
      "writing batch 26\n",
      "writing batch 27\n",
      "writing batch 28\n",
      "writing batch 29\n",
      "writing batch 30\n",
      "writing batch 31\n",
      "writing batch 32\n",
      "writing batch 33\n",
      "writing batch 34\n",
      "writing batch 35\n",
      "writing batch 36\n",
      "writing batch 37\n",
      "writing batch 38\n",
      "writing batch 39\n",
      "writing batch 40\n",
      "writing batch 41\n",
      "writing batch 42\n",
      "writing batch 43\n",
      "writing batch 44\n",
      "writing batch 45\n",
      "writing batch 46\n",
      "writing batch 47\n",
      "writing batch 48\n",
      "writing batch 49\n",
      "writing batch 50\n",
      "writing batch 51\n",
      "writing batch 52\n",
      "writing batch 53\n",
      "writing batch 54\n",
      "writing batch 55\n",
      "writing batch 56\n",
      "writing batch 57\n",
      "writing batch 58\n",
      "writing batch 59\n",
      "writing batch 60\n",
      "writing batch 61\n",
      "writing batch 62\n",
      "writing batch 63\n",
      "writing batch 64\n",
      "writing batch 65\n",
      "writing batch 66\n",
      "writing batch 67\n",
      "writing batch 68\n",
      "writing batch 69\n",
      "writing batch 70\n",
      "writing batch 71\n",
      "writing batch 72\n",
      "writing batch 73\n",
      "writing batch 74\n",
      "writing batch 75\n",
      "writing batch 76\n",
      "writing batch 77\n",
      "writing batch 78\n",
      "writing batch 79\n",
      "writing batch 80\n",
      "writing batch 81\n",
      "writing batch 82\n",
      "writing batch 83\n",
      "writing batch 84\n",
      "writing batch 85\n",
      "writing batch 86\n",
      "writing batch 87\n",
      "writing batch 88\n",
      "writing batch 89\n",
      "writing batch 90\n",
      "writing batch 91\n",
      "writing batch 92\n",
      "writing batch 93\n",
      "writing batch 94\n",
      "writing batch 95\n",
      "writing batch 96\n",
      "writing batch 97\n",
      "writing batch 98\n",
      "writing batch 99\n",
      "writing batch 100\n",
      "writing batch 101\n",
      "writing batch 102\n",
      "writing batch 103\n",
      "writing batch 104\n",
      "writing batch 105\n",
      "writing batch 106\n",
      "writing batch 107\n",
      "writing batch 108\n",
      "writing batch 109\n",
      "writing batch 110\n",
      "writing batch 111\n",
      "writing batch 112\n",
      "writing batch 113\n",
      "writing batch 114\n",
      "writing batch 115\n",
      "writing batch 116\n",
      "writing batch 117\n",
      "writing batch 118\n",
      "writing batch 119\n",
      "writing batch 120\n",
      "writing batch 121\n",
      "writing batch 122\n",
      "writing batch 123\n",
      "writing batch 124\n",
      "writing batch 125\n",
      "writing batch 126\n",
      "writing batch 127\n",
      "writing batch 128\n",
      "writing batch 129\n",
      "writing batch 130\n",
      "writing batch 131\n",
      "writing batch 132\n",
      "writing batch 133\n",
      "writing batch 134\n",
      "writing batch 135\n",
      "writing batch 136\n",
      "writing batch 137\n",
      "writing batch 138\n",
      "writing batch 139\n",
      "writing batch 140\n",
      "writing batch 141\n",
      "writing batch 142\n",
      "writing batch 143\n",
      "writing batch 144\n",
      "writing batch 145\n",
      "writing batch 146\n",
      "writing batch 147\n",
      "writing batch 148\n",
      "writing batch 149\n",
      "writing batch 150\n",
      "writing batch 151\n",
      "writing batch 152\n",
      "writing batch 153\n",
      "writing batch 154\n",
      "writing batch 155\n",
      "writing batch 156\n",
      "writing batch 157\n",
      "writing batch 158\n",
      "writing batch 159\n",
      "writing batch 160\n",
      "writing batch 161\n",
      "writing batch 162\n",
      "writing batch 163\n",
      "writing batch 164\n",
      "writing batch 165\n",
      "writing batch 166\n",
      "writing batch 167\n",
      "writing batch 168\n",
      "writing batch 169\n",
      "writing batch 170\n",
      "writing batch 171\n",
      "writing batch 172\n",
      "writing batch 173\n",
      "writing batch 174\n",
      "writing batch 175\n",
      "writing batch 176\n",
      "writing batch 177\n",
      "writing batch 178\n",
      "writing batch 179\n",
      "writing batch 180\n",
      "writing batch 181\n",
      "writing batch 182\n",
      "writing batch 183\n",
      "writing batch 184\n",
      "writing batch 185\n",
      "writing batch 186\n",
      "writing batch 187\n",
      "writing batch 188\n",
      "writing batch 189\n",
      "writing batch 190\n",
      "writing batch 191\n",
      "writing batch 192\n",
      "writing batch 193\n",
      "writing batch 194\n",
      "writing batch 195\n",
      "writing batch 196\n",
      "writing batch 197\n",
      "writing batch 198\n",
      "writing batch 199\n",
      "writing batch 200\n",
      "writing batch 201\n",
      "writing batch 202\n",
      "writing batch 203\n",
      "writing batch 204\n",
      "writing batch 205\n",
      "writing batch 206\n",
      "writing batch 207\n",
      "writing batch 208\n",
      "writing batch 209\n",
      "writing batch 210\n",
      "writing batch 211\n",
      "writing batch 212\n",
      "writing batch 213\n",
      "writing batch 214\n",
      "writing batch 215\n",
      "writing batch 216\n",
      "writing batch 217\n",
      "writing batch 218\n",
      "writing batch 219\n",
      "writing batch 220\n",
      "writing batch 221\n",
      "writing batch 222\n",
      "writing batch 223\n",
      "writing batch 224\n",
      "writing batch 225\n",
      "writing batch 226\n",
      "writing batch 227\n",
      "writing batch 228\n",
      "writing batch 229\n",
      "writing batch 230\n",
      "writing batch 231\n",
      "writing batch 232\n",
      "writing batch 233\n",
      "writing batch 234\n",
      "writing batch 235\n",
      "writing batch 236\n",
      "writing batch 237\n",
      "writing batch 238\n",
      "writing batch 239\n",
      "writing batch 240\n",
      "writing batch 241\n",
      "writing batch 242\n",
      "writing batch 243\n",
      "writing batch 244\n",
      "writing batch 245\n",
      "writing batch 246\n",
      "writing batch 247\n",
      "writing batch 248\n",
      "writing batch 249\n",
      "writing batch 250\n",
      "writing batch 251\n",
      "writing batch 252\n",
      "writing batch 253\n",
      "writing batch 254\n",
      "writing batch 255\n",
      "writing batch 256\n",
      "writing batch 257\n",
      "writing batch 258\n",
      "writing batch 259\n",
      "writing batch 260\n",
      "writing batch 261\n",
      "writing batch 262\n",
      "writing batch 263\n",
      "writing batch 264\n",
      "writing batch 265\n",
      "writing batch 266\n",
      "writing batch 267\n",
      "writing batch 268\n",
      "writing batch 269\n",
      "writing batch 270\n",
      "writing batch 271\n",
      "writing batch 272\n",
      "writing batch 273\n",
      "writing batch 274\n",
      "writing batch 275\n",
      "writing batch 276\n",
      "writing batch 277\n",
      "writing batch 278\n",
      "writing batch 279\n",
      "writing batch 280\n",
      "writing batch 281\n",
      "writing batch 282\n",
      "writing batch 283\n",
      "writing batch 284\n",
      "writing batch 285\n",
      "writing batch 286\n",
      "writing batch 287\n",
      "writing batch 288\n",
      "writing batch 289\n",
      "writing batch 290\n",
      "writing batch 291\n",
      "writing batch 292\n",
      "writing batch 293\n",
      "writing batch 294\n",
      "writing batch 295\n",
      "writing batch 296\n",
      "writing batch 297\n",
      "writing batch 298\n",
      "writing batch 299\n",
      "writing batch 300\n",
      "writing batch 301\n",
      "writing batch 302\n",
      "writing batch 303\n",
      "writing batch 304\n",
      "writing batch 305\n",
      "writing batch 306\n",
      "writing batch 307\n",
      "writing batch 308\n",
      "writing batch 309\n",
      "writing batch 310\n",
      "writing batch 311\n",
      "writing batch 312\n",
      "writing batch 313\n",
      "writing batch 314\n",
      "writing batch 315\n",
      "writing batch 316\n",
      "writing batch 317\n",
      "writing batch 318\n",
      "writing batch 319\n",
      "writing batch 320\n",
      "writing batch 321\n",
      "writing batch 322\n",
      "writing batch 323\n",
      "writing batch 324\n",
      "writing batch 325\n",
      "writing batch 326\n",
      "writing batch 327\n",
      "writing batch 328\n",
      "writing batch 329\n",
      "writing batch 330\n",
      "writing batch 331\n",
      "writing batch 332\n",
      "writing batch 333\n",
      "writing batch 334\n",
      "writing batch 335\n",
      "writing batch 336\n",
      "writing batch 337\n",
      "writing batch 338\n",
      "writing batch 339\n",
      "writing batch 340\n",
      "writing batch 341\n",
      "writing batch 342\n",
      "writing batch 343\n",
      "writing batch 344\n",
      "writing batch 345\n",
      "writing batch 346\n",
      "writing batch 347\n",
      "writing batch 348\n",
      "writing batch 349\n",
      "writing batch 350\n",
      "writing batch 351\n",
      "writing batch 352\n",
      "writing batch 353\n",
      "writing batch 354\n",
      "writing batch 355\n",
      "writing batch 356\n",
      "writing batch 357\n",
      "writing batch 358\n",
      "writing batch 359\n",
      "writing batch 360\n",
      "writing batch 361\n",
      "writing batch 362\n",
      "writing batch 363\n",
      "writing batch 364\n",
      "writing batch 365\n",
      "writing batch 366\n",
      "writing batch 367\n",
      "writing batch 368\n",
      "writing batch 369\n",
      "writing batch 370\n",
      "writing batch 371\n",
      "writing batch 372\n",
      "writing batch 373\n",
      "writing batch 374\n",
      "writing batch 375\n",
      "writing batch 376\n",
      "writing batch 377\n",
      "writing batch 378\n",
      "writing batch 379\n",
      "writing batch 380\n",
      "writing batch 381\n",
      "writing batch 382\n",
      "writing batch 383\n",
      "writing batch 384\n",
      "writing batch 385\n",
      "writing batch 386\n",
      "writing batch 387\n",
      "writing batch 388\n",
      "writing batch 389\n",
      "writing batch 390\n",
      "writing batch 391\n",
      "writing batch 392\n",
      "writing batch 393\n",
      "writing batch 394\n",
      "writing batch 395\n",
      "writing batch 396\n",
      "writing batch 397\n",
      "writing batch 398\n",
      "writing batch 399\n",
      "writing batch 400\n",
      "writing batch 401\n",
      "writing batch 402\n",
      "writing batch 403\n",
      "writing batch 404\n",
      "writing batch 405\n",
      "writing batch 406\n",
      "writing batch 407\n",
      "writing batch 408\n",
      "writing batch 409\n",
      "writing batch 410\n",
      "writing batch 411\n",
      "writing batch 412\n",
      "writing batch 413\n",
      "writing batch 414\n",
      "writing batch 415\n",
      "writing batch 416\n",
      "writing batch 417\n",
      "writing batch 418\n",
      "writing batch 419\n",
      "writing batch 420\n",
      "writing batch 421\n",
      "writing batch 422\n",
      "writing batch 423\n",
      "writing batch 424\n",
      "writing batch 425\n",
      "writing batch 426\n",
      "writing batch 427\n",
      "writing batch 428\n",
      "writing batch 429\n",
      "writing batch 430\n",
      "writing batch 431\n",
      "writing batch 432\n",
      "writing batch 433\n",
      "writing batch 434\n",
      "writing batch 435\n",
      "writing batch 436\n",
      "writing batch 437\n",
      "writing batch 438\n",
      "writing batch 439\n",
      "writing batch 440\n",
      "writing batch 441\n",
      "writing batch 442\n",
      "writing batch 443\n",
      "writing batch 444\n",
      "writing batch 445\n",
      "writing batch 446\n",
      "writing batch 447\n",
      "writing batch 448\n",
      "writing batch 449\n",
      "writing batch 450\n",
      "writing batch 451\n",
      "writing batch 452\n",
      "writing batch 453\n",
      "writing batch 454\n",
      "writing batch 455\n",
      "writing batch 456\n",
      "writing batch 457\n",
      "writing batch 458\n",
      "writing batch 459\n",
      "writing batch 460\n",
      "writing batch 461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 462\n",
      "writing batch 463\n",
      "writing batch 464\n",
      "writing batch 465\n",
      "writing batch 466\n",
      "writing batch 467\n",
      "writing batch 468\n",
      "writing batch 469\n",
      "writing batch 470\n",
      "writing batch 471\n",
      "writing batch 472\n",
      "writing batch 473\n",
      "writing batch 474\n",
      "writing batch 475\n",
      "writing batch 476\n",
      "writing batch 477\n",
      "writing batch 478\n",
      "writing batch 479\n",
      "writing batch 480\n",
      "writing batch 481\n",
      "writing batch 482\n",
      "writing batch 483\n",
      "writing batch 484\n",
      "writing batch 485\n",
      "writing batch 486\n",
      "writing batch 487\n",
      "writing batch 488\n",
      "writing batch 489\n",
      "writing batch 490\n",
      "writing batch 491\n",
      "writing batch 492\n",
      "writing batch 493\n",
      "writing batch 494\n",
      "writing batch 495\n",
      "writing batch 496\n",
      "writing batch 497\n",
      "writing batch 498\n",
      "writing batch 499\n",
      "writing batch 500\n",
      "writing batch 501\n",
      "writing batch 502\n",
      "writing batch 503\n",
      "writing batch 504\n",
      "writing batch 505\n",
      "writing batch 506\n",
      "writing batch 507\n",
      "writing batch 508\n",
      "writing batch 509\n",
      "writing batch 510\n",
      "writing batch 511\n",
      "writing batch 512\n",
      "writing batch 513\n",
      "writing batch 514\n",
      "writing batch 515\n",
      "writing batch 516\n",
      "writing batch 517\n",
      "writing batch 518\n",
      "writing batch 519\n",
      "writing batch 520\n",
      "writing batch 521\n",
      "writing batch 522\n",
      "writing batch 523\n",
      "writing batch 524\n",
      "writing batch 525\n",
      "writing batch 526\n",
      "writing batch 527\n",
      "writing batch 528\n",
      "writing batch 529\n",
      "writing batch 530\n",
      "writing batch 531\n",
      "writing batch 532\n",
      "writing batch 533\n",
      "writing batch 534\n",
      "writing batch 535\n",
      "writing batch 536\n",
      "writing batch 537\n",
      "writing batch 538\n",
      "writing batch 539\n",
      "writing batch 540\n",
      "writing batch 541\n",
      "writing batch 542\n",
      "writing batch 543\n",
      "writing batch 544\n",
      "writing batch 545\n",
      "writing batch 546\n",
      "writing batch 547\n",
      "writing batch 548\n",
      "writing batch 549\n",
      "writing batch 550\n",
      "writing batch 551\n",
      "writing batch 552\n",
      "writing batch 553\n",
      "writing batch 554\n",
      "writing batch 555\n",
      "writing batch 556\n",
      "writing batch 557\n",
      "writing batch 558\n",
      "writing batch 559\n",
      "writing batch 560\n",
      "writing batch 561\n",
      "writing batch 562\n",
      "writing batch 563\n",
      "writing batch 564\n",
      "writing batch 565\n",
      "writing batch 566\n",
      "writing batch 567\n",
      "writing batch 568\n",
      "writing batch 569\n",
      "writing batch 570\n",
      "writing batch 571\n",
      "writing batch 572\n",
      "writing batch 573\n",
      "writing batch 574\n",
      "writing batch 575\n",
      "writing batch 576\n",
      "writing batch 577\n",
      "writing batch 578\n",
      "writing batch 579\n",
      "writing batch 580\n",
      "writing batch 581\n",
      "writing batch 582\n",
      "writing batch 583\n",
      "writing batch 584\n",
      "writing batch 585\n",
      "writing batch 586\n",
      "writing batch 587\n",
      "writing batch 588\n",
      "writing batch 589\n",
      "writing batch 590\n",
      "writing batch 591\n",
      "writing batch 592\n",
      "writing batch 593\n",
      "writing batch 594\n",
      "writing batch 595\n",
      "writing batch 596\n",
      "writing batch 597\n",
      "writing batch 598\n",
      "writing batch 599\n",
      "writing batch 600\n",
      "writing batch 601\n",
      "writing batch 602\n",
      "writing batch 603\n",
      "writing batch 604\n",
      "writing batch 605\n",
      "writing batch 606\n",
      "writing batch 607\n",
      "writing batch 608\n",
      "writing batch 609\n",
      "writing batch 610\n",
      "writing batch 611\n",
      "writing batch 612\n",
      "writing batch 613\n",
      "writing batch 614\n",
      "writing batch 615\n",
      "writing batch 616\n",
      "writing batch 617\n",
      "writing batch 618\n",
      "writing batch 619\n",
      "writing batch 620\n",
      "writing batch 621\n",
      "writing batch 622\n",
      "writing batch 623\n",
      "writing batch 624\n",
      "writing batch 625\n",
      "writing batch 626\n",
      "writing batch 627\n",
      "writing batch 628\n",
      "writing batch 629\n",
      "writing batch 630\n",
      "writing batch 631\n",
      "writing batch 632\n",
      "writing batch 633\n",
      "writing batch 634\n",
      "writing batch 635\n",
      "writing batch 636\n",
      "writing batch 637\n",
      "writing batch 638\n",
      "writing batch 639\n",
      "writing batch 640\n",
      "writing batch 641\n",
      "writing batch 642\n",
      "writing batch 643\n",
      "writing batch 644\n",
      "writing batch 645\n",
      "writing batch 646\n",
      "writing batch 647\n",
      "writing batch 648\n",
      "writing batch 649\n",
      "writing batch 650\n",
      "writing batch 651\n",
      "writing batch 652\n",
      "writing batch 653\n",
      "writing batch 654\n",
      "writing batch 655\n",
      "writing batch 656\n",
      "writing batch 657\n",
      "writing batch 658\n",
      "writing batch 659\n",
      "writing batch 660\n",
      "writing batch 661\n",
      "writing batch 662\n",
      "writing batch 663\n",
      "writing batch 664\n",
      "writing batch 665\n",
      "writing batch 666\n",
      "writing batch 667\n",
      "writing batch 668\n",
      "writing batch 669\n",
      "writing batch 670\n",
      "writing batch 671\n",
      "writing batch 672\n",
      "writing batch 673\n",
      "writing batch 674\n",
      "writing batch 675\n",
      "writing batch 676\n",
      "writing batch 677\n",
      "writing batch 678\n",
      "writing batch 679\n",
      "writing batch 680\n",
      "writing batch 681\n",
      "writing batch 682\n",
      "writing batch 683\n",
      "writing batch 684\n",
      "writing batch 685\n",
      "writing batch 686\n",
      "writing batch 687\n",
      "writing batch 688\n",
      "writing batch 689\n",
      "writing batch 690\n",
      "writing batch 691\n",
      "writing batch 692\n",
      "writing batch 693\n",
      "writing batch 694\n",
      "writing batch 695\n",
      "writing batch 696\n",
      "writing batch 697\n",
      "writing batch 698\n",
      "writing batch 699\n",
      "writing batch 700\n",
      "writing batch 701\n",
      "writing batch 702\n",
      "writing batch 703\n",
      "writing batch 704\n",
      "writing batch 705\n",
      "writing batch 706\n",
      "writing batch 707\n",
      "writing batch 708\n",
      "writing batch 709\n",
      "writing batch 710\n",
      "writing batch 711\n",
      "writing batch 712\n",
      "writing batch 713\n",
      "writing batch 714\n",
      "writing batch 715\n",
      "writing batch 716\n",
      "writing batch 717\n",
      "writing batch 718\n",
      "writing batch 719\n",
      "writing batch 720\n",
      "writing batch 721\n",
      "writing batch 722\n",
      "writing batch 723\n",
      "writing batch 724\n",
      "writing batch 725\n",
      "writing batch 726\n",
      "writing batch 727\n",
      "writing batch 728\n",
      "writing batch 729\n",
      "writing batch 730\n",
      "writing batch 731\n",
      "writing batch 732\n",
      "writing batch 733\n",
      "writing batch 734\n",
      "writing batch 735\n",
      "writing batch 736\n",
      "writing batch 737\n",
      "writing batch 738\n",
      "writing batch 739\n",
      "writing batch 740\n",
      "writing batch 741\n",
      "writing batch 742\n",
      "writing batch 743\n",
      "writing batch 744\n",
      "writing batch 745\n",
      "writing batch 746\n",
      "writing batch 747\n",
      "writing batch 748\n",
      "writing batch 749\n",
      "writing batch 750\n",
      "writing batch 751\n",
      "writing batch 752\n",
      "writing batch 753\n",
      "writing batch 754\n",
      "writing batch 755\n",
      "writing batch 756\n",
      "writing batch 757\n",
      "writing batch 758\n",
      "writing batch 759\n",
      "writing batch 760\n",
      "writing batch 761\n",
      "writing batch 762\n",
      "writing batch 763\n",
      "writing batch 764\n",
      "writing batch 765\n",
      "writing batch 766\n",
      "writing batch 767\n",
      "writing batch 768\n",
      "writing batch 769\n",
      "writing batch 770\n",
      "writing batch 771\n",
      "writing batch 772\n",
      "writing batch 773\n",
      "writing batch 774\n",
      "writing batch 775\n",
      "writing batch 776\n",
      "writing batch 777\n",
      "writing batch 778\n",
      "writing batch 779\n",
      "writing batch 780\n",
      "writing batch 781\n",
      "writing batch 782\n",
      "writing batch 783\n",
      "writing batch 784\n",
      "writing batch 785\n",
      "writing batch 786\n",
      "writing batch 787\n",
      "writing batch 788\n",
      "writing batch 789\n",
      "writing batch 790\n",
      "writing batch 791\n",
      "writing batch 792\n",
      "writing batch 793\n",
      "writing batch 794\n",
      "writing batch 795\n",
      "writing batch 796\n",
      "writing batch 797\n",
      "writing batch 798\n",
      "writing batch 799\n",
      "writing batch 800\n",
      "writing batch 801\n",
      "writing batch 802\n",
      "writing batch 803\n",
      "writing batch 804\n",
      "writing batch 805\n",
      "writing batch 806\n",
      "writing batch 807\n",
      "writing batch 808\n",
      "writing batch 809\n",
      "writing batch 810\n",
      "writing batch 811\n",
      "writing batch 812\n",
      "writing batch 813\n",
      "writing batch 814\n",
      "writing batch 815\n",
      "writing batch 816\n",
      "writing batch 817\n",
      "writing batch 818\n",
      "writing batch 819\n",
      "writing batch 820\n",
      "writing batch 821\n",
      "writing batch 822\n",
      "writing batch 823\n",
      "writing batch 824\n",
      "writing batch 825\n",
      "writing batch 826\n",
      "writing batch 827\n",
      "writing batch 828\n",
      "writing batch 829\n",
      "writing batch 830\n",
      "writing batch 831\n",
      "writing batch 832\n",
      "writing batch 833\n",
      "writing batch 834\n",
      "writing batch 835\n",
      "writing batch 836\n",
      "writing batch 837\n",
      "writing batch 838\n",
      "writing batch 839\n",
      "writing batch 840\n",
      "writing batch 841\n",
      "writing batch 842\n",
      "writing batch 843\n",
      "writing batch 844\n",
      "writing batch 845\n",
      "writing batch 846\n",
      "writing batch 847\n",
      "writing batch 848\n",
      "writing batch 849\n",
      "writing batch 850\n",
      "writing batch 851\n",
      "writing batch 852\n",
      "writing batch 853\n",
      "writing batch 854\n",
      "writing batch 855\n",
      "writing batch 856\n",
      "writing batch 857\n",
      "writing batch 858\n",
      "writing batch 859\n",
      "writing batch 860\n",
      "writing batch 861\n",
      "writing batch 862\n",
      "writing batch 863\n",
      "writing batch 864\n",
      "writing batch 865\n",
      "writing batch 866\n",
      "writing batch 867\n",
      "writing batch 868\n",
      "writing batch 869\n",
      "writing batch 870\n",
      "writing batch 871\n",
      "writing batch 872\n",
      "writing batch 873\n",
      "writing batch 874\n",
      "writing batch 875\n",
      "writing batch 876\n",
      "writing batch 877\n",
      "writing batch 878\n",
      "writing batch 879\n",
      "writing batch 880\n",
      "writing batch 881\n",
      "writing batch 882\n",
      "writing batch 883\n",
      "writing batch 884\n",
      "writing batch 885\n",
      "writing batch 886\n",
      "writing batch 887\n",
      "writing batch 888\n",
      "writing batch 889\n",
      "writing batch 890\n",
      "writing batch 891\n",
      "writing batch 892\n",
      "writing batch 893\n",
      "writing batch 894\n",
      "writing batch 895\n",
      "writing batch 896\n",
      "writing batch 897\n",
      "writing batch 898\n",
      "writing batch 899\n",
      "writing batch 900\n",
      "writing batch 901\n",
      "writing batch 902\n",
      "writing batch 903\n",
      "writing batch 904\n",
      "writing batch 905\n",
      "writing batch 906\n",
      "writing batch 907\n",
      "writing batch 908\n",
      "writing batch 909\n",
      "writing batch 910\n",
      "writing batch 911\n",
      "writing batch 912\n",
      "writing batch 913\n",
      "writing batch 914\n",
      "writing batch 915\n",
      "writing batch 916\n",
      "writing batch 917\n",
      "writing batch 918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 919\n",
      "writing batch 920\n",
      "writing batch 921\n",
      "writing batch 922\n",
      "writing batch 923\n",
      "writing batch 924\n",
      "writing batch 925\n",
      "writing batch 926\n",
      "writing batch 927\n",
      "writing batch 928\n",
      "writing batch 929\n",
      "writing batch 930\n",
      "writing batch 931\n",
      "writing batch 932\n",
      "writing batch 933\n",
      "writing batch 934\n",
      "writing batch 935\n",
      "writing batch 936\n",
      "writing batch 937\n",
      "writing batch 938\n",
      "writing batch 939\n",
      "writing batch 940\n",
      "writing batch 941\n",
      "writing batch 942\n",
      "writing batch 943\n",
      "writing batch 944\n",
      "writing batch 945\n",
      "writing batch 946\n",
      "writing batch 947\n",
      "writing batch 948\n",
      "writing batch 949\n",
      "writing batch 950\n",
      "writing batch 951\n",
      "writing batch 952\n",
      "writing batch 953\n",
      "writing batch 954\n",
      "writing batch 955\n",
      "writing batch 956\n",
      "writing batch 957\n",
      "writing batch 958\n",
      "writing batch 959\n",
      "writing batch 960\n",
      "writing batch 961\n",
      "writing batch 962\n",
      "writing batch 963\n",
      "writing batch 964\n",
      "writing batch 965\n",
      "writing batch 966\n",
      "writing batch 967\n",
      "writing batch 968\n",
      "writing batch 969\n",
      "writing batch 970\n",
      "writing batch 971\n",
      "writing batch 972\n",
      "writing batch 973\n",
      "writing batch 974\n",
      "writing batch 975\n",
      "writing batch 976\n",
      "writing batch 977\n",
      "writing batch 978\n",
      "writing batch 979\n",
      "writing batch 980\n",
      "writing batch 981\n",
      "writing batch 982\n",
      "writing batch 983\n",
      "writing batch 984\n",
      "writing batch 985\n",
      "writing batch 986\n",
      "writing batch 987\n",
      "writing batch 988\n",
      "writing batch 989\n",
      "writing batch 990\n",
      "writing batch 991\n",
      "writing batch 992\n",
      "writing batch 993\n",
      "writing batch 994\n",
      "writing batch 995\n",
      "writing batch 996\n",
      "writing batch 997\n",
      "writing batch 998\n",
      "writing batch 999\n",
      "writing batch 1000\n",
      "writing batch 1001\n",
      "writing batch 1002\n",
      "writing batch 1003\n",
      "writing batch 1004\n",
      "writing batch 1005\n",
      "writing batch 1006\n",
      "writing batch 1007\n",
      "writing batch 1008\n",
      "writing batch 1009\n",
      "writing batch 1010\n",
      "writing batch 1011\n",
      "writing batch 1012\n",
      "writing batch 1013\n",
      "writing batch 1014\n",
      "writing batch 1015\n",
      "writing batch 1016\n",
      "writing batch 1017\n",
      "writing batch 1018\n",
      "writing batch 1019\n",
      "writing batch 1020\n",
      "writing batch 1021\n",
      "writing batch 1022\n",
      "writing batch 1023\n",
      "writing batch 1024\n",
      "writing batch 1025\n",
      "writing batch 1026\n",
      "writing batch 1027\n",
      "writing batch 1028\n",
      "writing batch 1029\n",
      "writing batch 1030\n",
      "writing batch 1031\n",
      "writing batch 1032\n",
      "writing batch 1033\n",
      "writing batch 1034\n",
      "writing batch 1035\n",
      "writing batch 1036\n",
      "writing batch 1037\n",
      "writing batch 1038\n",
      "writing batch 1039\n",
      "writing batch 1040\n",
      "writing batch 1041\n",
      "writing batch 1042\n",
      "writing batch 1043\n",
      "writing batch 1044\n",
      "writing batch 1045\n",
      "writing batch 1046\n",
      "writing batch 1047\n",
      "writing batch 1048\n",
      "writing batch 1049\n",
      "writing batch 1050\n",
      "writing batch 1051\n",
      "writing batch 1052\n",
      "writing batch 1053\n",
      "writing batch 1054\n",
      "writing batch 1055\n",
      "writing batch 1056\n",
      "writing batch 1057\n",
      "writing batch 1058\n",
      "writing batch 1059\n",
      "writing batch 1060\n",
      "writing batch 1061\n",
      "writing batch 1062\n",
      "writing batch 1063\n",
      "writing batch 1064\n",
      "writing batch 1065\n",
      "writing batch 1066\n",
      "writing batch 1067\n",
      "writing batch 1068\n",
      "writing batch 1069\n",
      "writing batch 1070\n",
      "writing batch 1071\n",
      "writing batch 1072\n",
      "writing batch 1073\n",
      "writing batch 1074\n",
      "writing batch 1075\n",
      "writing batch 1076\n",
      "writing batch 1077\n",
      "writing batch 1078\n",
      "writing batch 1079\n",
      "writing batch 1080\n",
      "writing batch 1081\n",
      "writing batch 1082\n",
      "writing batch 1083\n",
      "writing batch 1084\n",
      "writing batch 1085\n",
      "writing batch 1086\n",
      "writing batch 1087\n",
      "writing batch 1088\n",
      "writing batch 1089\n",
      "writing batch 1090\n",
      "writing batch 1091\n",
      "writing batch 1092\n",
      "writing batch 1093\n",
      "writing batch 1094\n",
      "writing batch 1095\n",
      "writing batch 1096\n",
      "writing batch 1097\n",
      "writing batch 1098\n",
      "writing batch 1099\n",
      "writing batch 1100\n",
      "writing batch 1101\n",
      "writing batch 1102\n",
      "writing batch 1103\n",
      "writing batch 1104\n",
      "writing batch 1105\n",
      "writing batch 1106\n",
      "writing batch 1107\n",
      "writing batch 1108\n",
      "writing batch 1109\n",
      "writing batch 1110\n",
      "writing batch 1111\n",
      "writing batch 1112\n",
      "writing batch 1113\n",
      "writing batch 1114\n",
      "writing batch 1115\n",
      "writing batch 1116\n",
      "writing batch 1117\n",
      "writing batch 1118\n",
      "writing batch 1119\n",
      "writing batch 1120\n",
      "writing batch 1121\n",
      "writing batch 1122\n",
      "writing batch 1123\n",
      "writing batch 1124\n",
      "writing batch 1125\n",
      "writing batch 1126\n",
      "writing batch 1127\n",
      "writing batch 1128\n",
      "writing batch 1129\n",
      "writing batch 1130\n",
      "writing batch 1131\n",
      "writing batch 1132\n",
      "writing batch 1133\n",
      "writing batch 1134\n",
      "writing batch 1135\n",
      "writing batch 1136\n",
      "writing batch 1137\n",
      "writing batch 1138\n",
      "writing batch 1139\n",
      "writing batch 1140\n",
      "writing batch 1141\n",
      "writing batch 1142\n",
      "writing batch 1143\n",
      "writing batch 1144\n",
      "writing batch 1145\n",
      "writing batch 1146\n",
      "writing batch 1147\n",
      "writing batch 1148\n",
      "writing batch 1149\n",
      "writing batch 1150\n",
      "writing batch 1151\n",
      "writing batch 1152\n",
      "writing batch 1153\n",
      "writing batch 1154\n",
      "writing batch 1155\n",
      "writing batch 1156\n",
      "writing batch 1157\n",
      "writing batch 1158\n",
      "writing batch 1159\n",
      "writing batch 1160\n",
      "writing batch 1161\n",
      "writing batch 1162\n",
      "writing batch 1163\n",
      "writing batch 1164\n",
      "writing batch 1165\n",
      "writing batch 1166\n",
      "writing batch 1167\n",
      "writing batch 1168\n",
      "writing batch 1169\n",
      "writing batch 1170\n",
      "writing batch 1171\n",
      "writing batch 1172\n",
      "writing batch 1173\n",
      "writing batch 1174\n",
      "writing batch 1175\n",
      "writing batch 1176\n",
      "writing batch 1177\n",
      "writing batch 1178\n",
      "writing batch 1179\n",
      "writing batch 1180\n",
      "writing batch 1181\n",
      "writing batch 1182\n",
      "writing batch 1183\n",
      "writing batch 1184\n",
      "writing batch 1185\n",
      "writing batch 1186\n",
      "writing batch 1187\n",
      "writing batch 1188\n",
      "writing batch 1189\n",
      "writing batch 1190\n",
      "writing batch 1191\n",
      "writing batch 1192\n",
      "writing batch 1193\n",
      "writing batch 1194\n",
      "writing batch 1195\n",
      "writing batch 1196\n",
      "writing batch 1197\n",
      "writing batch 1198\n",
      "writing batch 1199\n",
      "writing batch 1200\n",
      "writing batch 1201\n",
      "writing batch 1202\n",
      "writing batch 1203\n",
      "writing batch 1204\n",
      "writing batch 1205\n",
      "writing batch 1206\n",
      "writing batch 1207\n",
      "writing batch 1208\n",
      "writing batch 1209\n",
      "writing batch 1210\n",
      "writing batch 1211\n",
      "writing batch 1212\n",
      "writing batch 1213\n",
      "writing batch 1214\n",
      "writing batch 1215\n",
      "writing batch 1216\n",
      "writing batch 1217\n",
      "writing batch 1218\n",
      "writing batch 1219\n",
      "writing batch 1220\n",
      "writing batch 1221\n",
      "writing batch 1222\n",
      "writing batch 1223\n",
      "writing batch 1224\n",
      "writing batch 1225\n",
      "writing batch 1226\n",
      "writing batch 1227\n",
      "writing batch 1228\n",
      "writing batch 1229\n",
      "writing batch 1230\n",
      "writing batch 1231\n",
      "writing batch 1232\n",
      "writing batch 1233\n",
      "writing batch 1234\n",
      "writing batch 1235\n",
      "writing batch 1236\n",
      "writing batch 1237\n",
      "writing batch 1238\n",
      "writing batch 1239\n",
      "writing batch 1240\n",
      "writing batch 1241\n",
      "writing batch 1242\n",
      "writing batch 1243\n",
      "writing batch 1244\n",
      "writing batch 1245\n",
      "writing batch 1246\n",
      "writing batch 1247\n",
      "writing batch 1248\n",
      "writing batch 1249\n",
      "writing batch 1250\n",
      "writing batch 1251\n",
      "writing batch 1252\n",
      "writing batch 1253\n",
      "writing batch 1254\n",
      "writing batch 1255\n",
      "writing batch 1256\n",
      "writing batch 1257\n",
      "writing batch 1258\n",
      "writing batch 1259\n",
      "writing batch 1260\n",
      "writing batch 1261\n",
      "writing batch 1262\n",
      "writing batch 1263\n",
      "writing batch 1264\n",
      "writing batch 1265\n",
      "writing batch 1266\n",
      "writing batch 1267\n",
      "writing batch 1268\n",
      "writing batch 1269\n",
      "writing batch 1270\n",
      "writing batch 1271\n",
      "writing batch 1272\n",
      "writing batch 1273\n",
      "writing batch 1274\n",
      "writing batch 1275\n",
      "writing batch 1276\n",
      "writing batch 1277\n",
      "writing batch 1278\n",
      "writing batch 1279\n",
      "writing batch 1280\n",
      "writing batch 1281\n",
      "writing batch 1282\n",
      "writing batch 1283\n",
      "writing batch 1284\n",
      "writing batch 1285\n",
      "writing batch 1286\n",
      "writing batch 1287\n",
      "writing batch 1288\n",
      "writing batch 1289\n",
      "writing batch 1290\n",
      "writing batch 1291\n",
      "writing batch 1292\n",
      "writing batch 1293\n",
      "writing batch 1294\n",
      "writing batch 1295\n",
      "writing batch 1296\n",
      "writing batch 1297\n",
      "writing batch 1298\n",
      "writing batch 1299\n",
      "writing batch 1300\n",
      "writing batch 1301\n",
      "writing batch 1302\n",
      "writing batch 1303\n",
      "writing batch 1304\n",
      "writing batch 1305\n",
      "writing batch 1306\n",
      "writing batch 1307\n",
      "writing batch 1308\n",
      "writing batch 1309\n",
      "writing batch 1310\n",
      "writing batch 1311\n",
      "writing batch 1312\n",
      "writing batch 1313\n",
      "writing batch 1314\n",
      "writing batch 1315\n",
      "writing batch 1316\n",
      "writing batch 1317\n",
      "writing batch 1318\n",
      "writing batch 1319\n",
      "writing batch 1320\n",
      "writing batch 1321\n",
      "writing batch 1322\n",
      "writing batch 1323\n",
      "writing batch 1324\n",
      "writing batch 1325\n",
      "writing batch 1326\n",
      "writing batch 1327\n",
      "writing batch 1328\n",
      "writing batch 1329\n",
      "writing batch 1330\n",
      "writing batch 1331\n",
      "writing batch 1332\n",
      "writing batch 1333\n",
      "writing batch 1334\n",
      "writing batch 1335\n",
      "writing batch 1336\n",
      "writing batch 1337\n",
      "writing batch 1338\n",
      "writing batch 1339\n",
      "writing batch 1340\n",
      "writing batch 1341\n",
      "writing batch 1342\n",
      "writing batch 1343\n",
      "writing batch 1344\n",
      "writing batch 1345\n",
      "writing batch 1346\n",
      "writing batch 1347\n",
      "writing batch 1348\n",
      "writing batch 1349\n",
      "writing batch 1350\n",
      "writing batch 1351\n",
      "writing batch 1352\n",
      "writing batch 1353\n",
      "writing batch 1354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 1355\n",
      "writing batch 1356\n",
      "writing batch 1357\n",
      "writing batch 1358\n",
      "writing batch 1359\n",
      "writing batch 1360\n",
      "writing batch 1361\n",
      "writing batch 1362\n",
      "writing batch 1363\n",
      "writing batch 1364\n",
      "writing batch 1365\n",
      "writing batch 1366\n",
      "writing batch 1367\n",
      "writing batch 1368\n",
      "writing batch 1369\n",
      "writing batch 1370\n",
      "writing batch 1371\n",
      "writing batch 1372\n",
      "writing batch 1373\n",
      "writing batch 1374\n",
      "writing batch 1375\n",
      "writing batch 1376\n",
      "writing batch 1377\n",
      "writing batch 1378\n",
      "writing batch 1379\n",
      "writing batch 1380\n",
      "writing batch 1381\n",
      "writing batch 1382\n",
      "writing batch 1383\n",
      "writing batch 1384\n",
      "writing batch 1385\n",
      "writing batch 1386\n",
      "writing batch 1387\n",
      "writing batch 1388\n",
      "writing batch 1389\n",
      "writing batch 1390\n",
      "writing batch 1391\n",
      "writing batch 1392\n",
      "writing batch 1393\n",
      "writing batch 1394\n",
      "writing batch 1395\n",
      "writing batch 1396\n",
      "writing batch 1397\n",
      "writing batch 1398\n",
      "writing batch 1399\n",
      "writing batch 1400\n",
      "writing batch 1401\n",
      "writing batch 1402\n",
      "writing batch 1403\n",
      "writing batch 1404\n",
      "writing batch 1405\n",
      "writing batch 1406\n",
      "writing batch 1407\n",
      "writing batch 1408\n",
      "writing batch 1409\n",
      "writing batch 1410\n",
      "writing batch 1411\n",
      "writing batch 1412\n",
      "writing batch 1413\n",
      "writing batch 1414\n",
      "writing batch 1415\n",
      "writing batch 1416\n",
      "writing batch 1417\n",
      "writing batch 1418\n",
      "writing batch 1419\n",
      "writing batch 1420\n",
      "writing batch 1421\n",
      "writing batch 1422\n",
      "writing batch 1423\n",
      "writing batch 1424\n",
      "writing batch 1425\n",
      "writing batch 1426\n",
      "writing batch 1427\n",
      "writing batch 1428\n",
      "writing batch 1429\n",
      "writing batch 1430\n",
      "writing batch 1431\n",
      "writing batch 1432\n",
      "writing batch 1433\n",
      "writing batch 1434\n",
      "writing batch 1435\n",
      "writing batch 1436\n",
      "writing batch 1437\n",
      "writing batch 1438\n",
      "writing batch 1439\n",
      "writing batch 1440\n",
      "writing batch 1441\n",
      "writing batch 1442\n",
      "writing batch 1443\n",
      "writing batch 1444\n",
      "writing batch 1445\n",
      "writing batch 1446\n",
      "writing batch 1447\n",
      "writing batch 1448\n",
      "writing batch 1449\n",
      "writing batch 1450\n",
      "writing batch 1451\n",
      "writing batch 1452\n",
      "writing batch 1453\n",
      "writing batch 1454\n",
      "writing batch 1455\n",
      "writing batch 1456\n",
      "writing batch 1457\n",
      "writing batch 1458\n",
      "writing batch 1459\n",
      "writing batch 1460\n",
      "writing batch 1461\n",
      "writing batch 1462\n",
      "writing batch 1463\n",
      "writing batch 1464\n",
      "writing batch 1465\n",
      "writing batch 1466\n",
      "writing batch 1467\n",
      "writing batch 1468\n",
      "writing batch 1469\n",
      "writing batch 1470\n",
      "writing batch 1471\n",
      "writing batch 1472\n",
      "writing batch 1473\n",
      "writing batch 1474\n",
      "writing batch 1475\n",
      "writing batch 1476\n",
      "writing batch 1477\n",
      "writing batch 1478\n",
      "writing batch 1479\n",
      "writing batch 1480\n",
      "writing batch 1481\n",
      "writing batch 1482\n",
      "writing batch 1483\n",
      "writing batch 1484\n",
      "writing batch 1485\n",
      "writing batch 1486\n",
      "writing batch 1487\n",
      "writing batch 1488\n",
      "writing batch 1489\n",
      "writing batch 1490\n",
      "writing batch 1491\n",
      "writing batch 1492\n",
      "writing batch 1493\n",
      "writing batch 1494\n",
      "writing batch 1495\n",
      "writing batch 1496\n",
      "writing batch 1497\n",
      "writing batch 1498\n",
      "writing batch 1499\n",
      "writing batch 1500\n",
      "writing batch 1501\n",
      "writing batch 1502\n",
      "writing batch 1503\n",
      "writing batch 1504\n",
      "writing batch 1505\n",
      "writing batch 1506\n",
      "writing batch 1507\n",
      "writing batch 1508\n",
      "writing batch 1509\n",
      "writing batch 1510\n",
      "writing batch 1511\n",
      "writing batch 1512\n",
      "writing batch 1513\n",
      "writing batch 1514\n",
      "writing batch 1515\n",
      "writing batch 1516\n",
      "writing batch 1517\n",
      "writing batch 1518\n",
      "writing batch 1519\n",
      "writing batch 1520\n",
      "writing batch 1521\n",
      "writing batch 1522\n",
      "writing batch 1523\n",
      "writing batch 1524\n",
      "writing batch 1525\n",
      "writing batch 1526\n",
      "writing batch 1527\n",
      "writing batch 1528\n",
      "writing batch 1529\n",
      "writing batch 1530\n",
      "writing batch 1531\n",
      "writing batch 1532\n",
      "writing batch 1533\n",
      "writing batch 1534\n",
      "writing batch 1535\n",
      "writing batch 1536\n",
      "writing batch 1537\n",
      "writing batch 1538\n",
      "writing batch 1539\n",
      "writing batch 1540\n",
      "writing batch 1541\n",
      "writing batch 1542\n",
      "writing batch 1543\n",
      "writing batch 1544\n",
      "writing batch 1545\n",
      "writing batch 1546\n",
      "writing batch 1547\n",
      "writing batch 1548\n",
      "writing batch 1549\n",
      "writing batch 1550\n",
      "writing batch 1551\n",
      "writing batch 1552\n",
      "writing batch 1553\n",
      "writing batch 1554\n",
      "writing batch 1555\n",
      "writing batch 1556\n",
      "writing batch 1557\n",
      "writing batch 1558\n",
      "writing batch 1559\n",
      "writing batch 1560\n",
      "writing batch 1561\n",
      "writing batch 1562\n",
      "writing batch 1563\n",
      "writing batch 1564\n",
      "writing batch 1565\n",
      "writing batch 1566\n",
      "writing batch 1567\n",
      "writing batch 1568\n",
      "writing batch 1569\n",
      "writing batch 1570\n",
      "writing batch 1571\n",
      "writing batch 1572\n",
      "writing batch 1573\n",
      "writing batch 1574\n",
      "writing batch 1575\n",
      "writing batch 1576\n",
      "writing batch 1577\n",
      "writing batch 1578\n",
      "writing batch 1579\n",
      "writing batch 1580\n",
      "writing batch 1581\n",
      "writing batch 1582\n",
      "writing batch 1583\n",
      "writing batch 1584\n",
      "writing batch 1585\n",
      "writing batch 1586\n",
      "writing batch 1587\n",
      "writing batch 1588\n",
      "writing batch 1589\n",
      "writing batch 1590\n",
      "writing batch 1591\n",
      "writing batch 1592\n",
      "writing batch 1593\n",
      "writing batch 1594\n",
      "writing batch 1595\n",
      "writing batch 1596\n",
      "writing batch 1597\n",
      "writing batch 1598\n",
      "writing batch 1599\n"
     ]
    }
   ],
   "source": [
    "for i in range(1600):\n",
    "    print(f'writing batch {i}')\n",
    "    mmap_tmp = np.memmap(f'{DATA_DIR}/tweet_vectors.npy', dtype=DTYPE, mode='r+',\n",
    "                         shape=(BATCHSIZE, MAXWIDTH, VECDIM),\n",
    "                         offset = i * BATCHSIZE * MAXWIDTH * VECDIM * DTYPE_BYTES)\n",
    "    mmap_tmp[:] = sequence.pad_sequences(tweets.vectors[(i*BATCHSIZE):((i+1)*BATCHSIZE)], \n",
    "                                         dtype=DTYPE, maxlen=MAXWIDTH, padding='post')\n",
    "    mmap_tmp.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.drop(columns=['tweet'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incl_vert_batches = Dataset.from_tensor_slices(incl_verts_for_reg).batch(BATCH_SIZE).prefetch(BATCH_SIZE)\n",
    "img_batches = Dataset.from_tensor_slices(img_vals).batch(BATCH_SIZE).prefetch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_write(batches, model, vert_count, frames_npy):\n",
    "    for i, batch in batches.enumerate().as_numpy_iterator():\n",
    "        batch_size = batch.shape[0]\n",
    "        batch_offset = i * BATCH_SIZE\n",
    "        output_shape = (batch_size, vert_count, VERTEX_DIM)\n",
    "        pred_positions = model.predict(batch).reshape(output_shape)\n",
    "        output_frames = np.memmap(frames_npy, dtype=DTYPE, mode='r+', shape = output_shape,\n",
    "                                  offset = batch_offset * vert_count * VERTEX_DIM * DTYPE_BYTES)\n",
    "        output_frames[:] = pred_positions\n",
    "        output_frames.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(vals, vals_label):\n",
    "    print(f'{vals_label}: mean = {vals.mean()}, var = {vals.var()}, min = {vals.min()}, ' +\n",
    "          f'median = {np.median(vals)}, max = {vals.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning under time constraints: based on data subsample\n",
    "I know this isn't kosher, but I have a deadline, and hopefully it works well enough for my present purposes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select random subsample for \"cross-validation\"\n",
    "I tried to sample random indices at one go, then select the indexed observations, but my kernel kept dying. Iterating over smaller chunks then aggregating the results seems to be more manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_train_x_chunks, tuning_train_y_chunks = [], []\n",
    "increment, iter_n = 200000, 8000\n",
    "for i in range(8):\n",
    "    start = i * increment\n",
    "    tuning_train_idx = np.random.randint(start, start + increment, size=iter_n)\n",
    "    tuning_train_x_chunks.append(padded_vectors[tuning_train_idx])\n",
    "    y = tweets.positive.iloc[tuning_train_idx].values\n",
    "    tuning_train_y_chunks.append(np.reshape(y, (y.shape[0], -1)))\n",
    "tuning_train_x = np.vstack(tuple(tuning_train_x_chunks))\n",
    "tuning_train_y = np.vstack(tuple(tuning_train_y_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tuning_train_x_chunks, tuning_train_y_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_shuffled_idxs = np.arange(tuning_train_x.shape[0])\n",
    "np.random.shuffle(tuning_shuffled_idxs)\n",
    "tuning_train_x = tuning_train_x[tuning_shuffled_idxs]\n",
    "tuning_train_y = tuning_train_y[tuning_shuffled_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(lstm_units, lstm_do, lstm_recurr_do, dense_units, dense_activ, dense_do, opt):\n",
    "    #strategy = tf.distribute.MirroredStrategy()\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(lstm_units, return_sequences=False, dropout=lstm_do, recurrent_dropout=lstm_recurr_do))\n",
    "    model.add(Dense(dense_units, activation=dense_activ))\n",
    "    model.add(Dropout(dense_do))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_indices(i, k, index):\n",
    "    if i < 1 or i > k:\n",
    "        raise ValueError\n",
    "    step, n = 1.0 / k, len(index)\n",
    "    lb, ub = round((i-1) * step * n), round(i * step * n)\n",
    "    include = list(range(lb))\n",
    "    include.extend(range(ub, n)) \n",
    "    exclude = list(range(lb, ub))\n",
    "    return index[exclude], index[include]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Not really k-fold CV, since the model isn't reinstantiated (i.e. weights are carried over from previous\n",
    "## fold trainings), but c.f. time constraints\n",
    "# def k_fold_cv(x, y, k, model, cv_verbose=False, **kwargs):\n",
    "#     accuracy = np.zeros(k)\n",
    "#     stdevs = np.zeros(k)\n",
    "#     for i in range(1, k+1):\n",
    "#         if cv_verbose:\n",
    "#             print('fold ', i)\n",
    "#         exclude, include = partition_indices(i, k, np.arange(x.shape[0]))\n",
    "#         x_fit = x[include]\n",
    "#         model.fit(x_fit, y[include], **kwargs)\n",
    "#         exclude_len = len(x[exclude])\n",
    "#         predictions = np.round(model.predict(x[exclude]))\n",
    "#         corrects = (predictions == y[exclude]).astype(int)\n",
    "#         accuracy[i-1] = corrects.mean()\n",
    "#         stdevs[i-1] = corrects.std()\n",
    "#     return accuracy, stdevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv_iter(x, y, i, k, model, cv_verbose=False, **kwargs):\n",
    "    exclude, include = partition_indices(i, k, np.arange(x.shape[0]))\n",
    "    x_fit = x[include]\n",
    "    model.fit(x_fit, y[include], **kwargs)\n",
    "    exclude_len = len(x[exclude])\n",
    "    predictions = np.round(model.predict(x[exclude]))\n",
    "    return (predictions == y[exclude]).astype(int).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate train-val-test indices, load data memmap, and pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(12)\n",
    "TRAINVAL_IDXS = rng.choice(SAMPLE_SIZE, size=int(0.85*SAMPLE_SIZE), replace=False, shuffle=False)\n",
    "VAL_IDXS = rng.choice(TRAINVAL_IDXS, size=int(0.15*SAMPLE_SIZE), replace=False, shuffle=False)\n",
    "TRAIN_IDXS = np.setdiff1d(TRAINVAL_IDXS, VAL_IDXS)\n",
    "TEST_IDXS = np.setdiff1d(np.arange(SAMPLE_SIZE), TRAINVAL_IDXS)\n",
    "TRAINVAL_IDXS.sort()\n",
    "TRAIN_IDXS.sort()\n",
    "VAL_IDXS.sort()\n",
    "TEST_IDXS.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_vectors = np.memmap(f'{DATA_DIR}/tweet_vectors.npy', dtype=DTYPE, mode='r',\n",
    "                           shape=(SAMPLE_SIZE, MAXWIDTH, VECDIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TUPLES = (padded_vectors[TRAIN_IDXS], tweets.positive[TRAIN_IDXS])\n",
    "train_tuples = Dataset.from_tensor_slices(TRAIN_TUPLES).shuffle(len(TRAIN_IDXS))\n",
    "train_batches = train_tuples.batch(BATCH_SIZE).prefetch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "model = get_model(128, 0.1, 0.1, 32, 'relu', 0.5, 'RMSprop')\n",
    "model_chkpt = callbacks.ModelCheckpoint(f'{CHKPT_DIR}/pass1', monitor='val_loss', save_best_only=True)\n",
    "csv_logger = callbacks.CSVLogger(f'{LOG_DIR}/pass1.csv', append=True),\n",
    "history = model.fit(padded_vectors[TRAIN_IDXS], tweets.positive[TRAIN_IDXS], epochs=10, batch_size=64,\n",
    "                    validation_data=padded_vectors[VAL_IDXS], callbacks=[model_chkpt, csv_logger])\n",
    "predictions = model.predict(padded_vectors[TEST_IDXS])\n",
    "predictions_rd = np.round(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = np.zeros(5)\n",
    "for i in range(1, 6):\n",
    "    print('fold ', i)\n",
    "    accuracies[i-1] = k_fold_cv_iter(tuning_train_x, tuning_train_y, i, 5,\n",
    "                                     get_model(128, 0.1, 0.1, 32, 'relu', 0.5, 'RMSprop'), epochs=3, batch_size=1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold  1\n",
      "Train on 51200 samples\n",
      "Epoch 1/3\n",
      "51200/51200 [==============================] - 60s 1ms/sample - loss: 5.9437e-08 - accuracy: 0.4938\n",
      "Epoch 2/3\n",
      "51200/51200 [==============================] - 57s 1ms/sample - loss: 5.9437e-08 - accuracy: 0.4946\n",
      "Epoch 3/3\n",
      "51200/51200 [==============================] - 57s 1ms/sample - loss: 5.9437e-08 - accuracy: 0.4937\n",
      "fold  2\n",
      "Train on 51200 samples\n",
      "Epoch 1/3\n",
      "51200/51200 [==============================] - 60s 1ms/sample - loss: 5.9621e-08 - accuracy: 0.4947\n",
      "Epoch 2/3\n",
      "51200/51200 [==============================] - 56s 1ms/sample - loss: 5.9621e-08 - accuracy: 0.4967\n",
      "Epoch 3/3\n",
      "51200/51200 [==============================] - 57s 1ms/sample - loss: 5.9621e-08 - accuracy: 0.4928\n",
      "fold  3\n",
      "Train on 51200 samples\n",
      "Epoch 1/3\n",
      "51200/51200 [==============================] - 61s 1ms/sample - loss: 5.9719e-08 - accuracy: 0.5048\n",
      "Epoch 2/3\n",
      "51200/51200 [==============================] - 57s 1ms/sample - loss: 5.9719e-08 - accuracy: 0.5024\n",
      "Epoch 3/3\n",
      "51200/51200 [==============================] - 56s 1ms/sample - loss: 5.9719e-08 - accuracy: 0.5052\n",
      "fold  4\n",
      "Train on 51200 samples\n",
      "Epoch 1/3\n",
      "51200/51200 [==============================] - 61s 1ms/sample - loss: 5.9551e-08 - accuracy: 0.5062\n",
      "Epoch 2/3\n",
      "51200/51200 [==============================] - 57s 1ms/sample - loss: 5.9551e-08 - accuracy: 0.5046\n",
      "Epoch 3/3\n",
      "51200/51200 [==============================] - 57s 1ms/sample - loss: 5.9551e-08 - accuracy: 0.5046\n",
      "fold  5\n",
      "Train on 51200 samples\n",
      "Epoch 1/3\n",
      "51200/51200 [==============================] - 63s 1ms/sample - loss: 5.9695e-08 - accuracy: 0.5094\n",
      "Epoch 2/3\n",
      "51200/51200 [==============================] - 60s 1ms/sample - loss: 5.9695e-08 - accuracy: 0.5093\n",
      "Epoch 3/3\n",
      "51200/51200 [==============================] - 58s 1ms/sample - loss: 5.9695e-08 - accuracy: 0.5101\n"
     ]
    }
   ],
   "source": [
    "accuracies = np.zeros(5)\n",
    "for i in range(1, 6):\n",
    "    print('fold ', i)\n",
    "    accuracies[i-1] = k_fold_cv_iter(tuning_train_x, tuning_train_y, i, 5,\n",
    "                                     get_model(128, 0.1, 0.1, 32, 'relu', 0.5, 'RMSprop'), epochs=3, batch_size=1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50976562, 0.49148437, 0.51351563, 0.5146875 , 0.5190625 ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold  1\n",
      "Train on 51200 samples\n",
      "Epoch 1/3\n",
      "51200/51200 [==============================] - 74s 1ms/sample - loss: 5.9535e-08 - accuracy: 0.5137\n",
      "Epoch 2/3\n",
      "51200/51200 [==============================] - 61s 1ms/sample - loss: 5.9535e-08 - accuracy: 0.5108\n",
      "Epoch 3/3\n",
      "51200/51200 [==============================] - 59s 1ms/sample - loss: 5.9535e-08 - accuracy: 0.5116\n",
      "fold  2\n",
      "Train on 51200 samples\n",
      "Epoch 1/3\n",
      "51200/51200 [==============================] - 61s 1ms/sample - loss: 5.9626e-08 - accuracy: 0.5096\n",
      "Epoch 2/3\n",
      "51200/51200 [==============================] - 58s 1ms/sample - loss: 5.9626e-08 - accuracy: 0.5101\n",
      "Epoch 3/3\n",
      "51200/51200 [==============================] - 58s 1ms/sample - loss: 5.9626e-08 - accuracy: 0.5105\n",
      "fold  3\n",
      "Train on 51200 samples\n",
      "Epoch 1/3\n",
      "51200/51200 [==============================] - 63s 1ms/sample - loss: 5.9728e-08 - accuracy: 0.5025\n",
      "Epoch 2/3\n",
      "51200/51200 [==============================] - 57s 1ms/sample - loss: 5.9728e-08 - accuracy: 0.5000\n",
      "Epoch 3/3\n",
      "51200/51200 [==============================] - 58s 1ms/sample - loss: 5.9728e-08 - accuracy: 0.4987\n",
      "fold  4\n",
      "Train on 51200 samples\n",
      "Epoch 1/3\n",
      "51200/51200 [==============================] - 63s 1ms/sample - loss: 5.9567e-08 - accuracy: 0.5188\n",
      "Epoch 2/3\n",
      "51200/51200 [==============================] - 58s 1ms/sample - loss: 5.9567e-08 - accuracy: 0.5189\n",
      "Epoch 3/3\n",
      "51200/51200 [==============================] - 59s 1ms/sample - loss: 5.9567e-08 - accuracy: 0.5198\n",
      "fold  5\n",
      "Train on 51200 samples\n",
      "Epoch 1/3\n",
      "51200/51200 [==============================] - 64s 1ms/sample - loss: 5.9567e-08 - accuracy: 0.4884\n",
      "Epoch 2/3\n",
      "51200/51200 [==============================] - 58s 1ms/sample - loss: 5.9567e-08 - accuracy: 0.4861\n",
      "Epoch 3/3\n",
      "51200/51200 [==============================] - 60s 1ms/sample - loss: 5.9567e-08 - accuracy: 0.4923\n"
     ]
    }
   ],
   "source": [
    "accuracies = np.zeros(5)\n",
    "for i in range(1, 6):\n",
    "    print('fold ', i)\n",
    "    accuracies[i-1] = k_fold_cv_iter(tuning_train_x, tuning_train_y, i, 5,\n",
    "                                     get_model(128, 0.1, 0.1, 64, 'relu', 0.5, 'RMSprop'), epochs=3, batch_size=1600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 13/6/2020: Maybe that subsampling idea was dumb? I've spent much of my time trying to get the kernel not to die, and gotten not much farther from where I started. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNDER CONSTRUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53523438, 0.50351563, 0.50929687, 0.52984375, 0.45015625])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold  1\n",
      "Train on 51200 samples\n",
      "Epoch 1/3\n",
      "51200/51200 [==============================] - 66s 1ms/sample - loss: 5.9535e-08 - accuracy: 0.5163\n",
      "Epoch 2/3\n",
      "51200/51200 [==============================] - 59s 1ms/sample - loss: 5.9535e-08 - accuracy: 0.5185\n",
      "Epoch 3/3\n",
      "51200/51200 [==============================] - 58s 1ms/sample - loss: 5.9535e-08 - accuracy: 0.5183\n",
      "fold  2\n",
      "Train on 51200 samples\n",
      "Epoch 1/3\n",
      "51200/51200 [==============================] - 63s 1ms/sample - loss: 5.9626e-08 - accuracy: 0.4826\n",
      "Epoch 2/3\n",
      "51200/51200 [==============================] - 57s 1ms/sample - loss: 5.9626e-08 - accuracy: 0.4824\n",
      "Epoch 3/3\n",
      "51200/51200 [==============================] - 57s 1ms/sample - loss: 5.9626e-08 - accuracy: 0.4837\n",
      "fold  3\n",
      "Train on 51200 samples\n",
      "Epoch 1/3\n",
      "51200/51200 [==============================] - 64s 1ms/sample - loss: 5.9728e-08 - accuracy: 0.4907\n",
      "Epoch 2/3\n",
      "51200/51200 [==============================] - 59s 1ms/sample - loss: 5.9728e-08 - accuracy: 0.4903\n",
      "Epoch 3/3\n",
      "51200/51200 [==============================] - 59s 1ms/sample - loss: 5.9728e-08 - accuracy: 0.4917\n",
      "fold  4\n"
     ]
    }
   ],
   "source": [
    "accuracies = np.zeros(5)\n",
    "for i in range(1, 6):\n",
    "    print('fold ', i)\n",
    "    accuracies[i-1] = k_fold_cv_iter(tuning_train_x, tuning_train_y, i, 5,\n",
    "                                     get_model(128, 0.1, 0.1, 32, 'relu', 0.2, 'RMSprop'),\n",
    "                                     epochs=3, batch_size=1600, workers=8, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM (64, 0.1, 0.1), dense (64, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adadelta: 0.4037, 0.4049, 0.4066, 0.35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSprop: 0.4469, 0.4444, 0.4479, 0.34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM (128, 0.1), dense (32, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSprop: 0.4952, 0.5015, 0.4979, 0.46"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom_python38",
   "language": "python",
   "name": "custom_python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
