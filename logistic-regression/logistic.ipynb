{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of the weights in logistic regression differs from the interpretation of the weights in linear regression, since the outcome in logistic regression is a probability between 0 and 1. The weights do not influence the probability linearly any longer. The weighted sum is transformed by the logistic function to a probability. Therefore we need to reformulate the equation for the interpretation so that only the linear term is on the right side of the formula. To be explicity, let's use $\\mathbb{P}(Y_i = 1) = \\pi_i$:\n",
    "\n",
    "$$\n",
    "log\\left(\\frac{\\mathbb{P}(y=1)}{1-\\mathbb{P}(y=1)}\\right)=log\\left(\\frac{\\mathbb{P}(y=1)}{\\mathbb{P}(y=0)}\\right)=\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{p}x_{p}\n",
    "$$\n",
    "\n",
    "Exponentiating both sides leads us to the odds:\n",
    "\n",
    "$$\n",
    "\\frac{\\mathbb{P}(y=1)}{1-\\mathbb{P}(y=1)}=odds=exp\\left(\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{p}x_{p}\\right)\n",
    "$$\n",
    "\n",
    "Then we compare what happens when we increase one of the feature values by 1. But instead of looking at the difference, we look at the ratio of the two predictions:\n",
    "\n",
    "$$\n",
    "\\frac{odds_{x_j+1}}{odds}=\\frac{exp\\left(\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{j}(x_{j}+1)+\\ldots+\\beta_{p}x_{p}\\right)}{exp\\left(\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{j}x_{j}+\\ldots+\\beta_{p}x_{p}\\right)}\n",
    "$$\n",
    "\n",
    "We apply the following rule:\n",
    "\n",
    "$$\n",
    "\\frac{exp(a)}{exp(b)}=exp(a-b)\n",
    "$$\n",
    "\n",
    "And we remove many terms:\n",
    "\n",
    "$$\n",
    "\\frac{odds_{x_j+1}}{odds}=exp\\left(\\beta_{j}(x_{j}+1)-\\beta_{j}x_{j}\\right)=exp\\left(\\beta_j\\right)\n",
    "$$\n",
    "\n",
    "In the end, we have something as simple as exp() of a feature weight. A change in a feature by one unit changes the odds ratio (multiplicative) by a factor of exp(Î²j). We could also interpret it this way: A change in xj by one unit increases the log odds ratio by the value of the corresponding weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "* How do we interpret the intercept?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.000000e+02</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.569154e+07</td>\n",
       "      <td>37.655000</td>\n",
       "      <td>69742.500000</td>\n",
       "      <td>0.357500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.165832e+04</td>\n",
       "      <td>10.482877</td>\n",
       "      <td>34096.960282</td>\n",
       "      <td>0.479864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.556669e+07</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>15000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.562676e+07</td>\n",
       "      <td>29.750000</td>\n",
       "      <td>43000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.569434e+07</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.575036e+07</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>88000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.581524e+07</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>150000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            User ID         Age  EstimatedSalary   Purchased\n",
       "count  4.000000e+02  400.000000       400.000000  400.000000\n",
       "mean   1.569154e+07   37.655000     69742.500000    0.357500\n",
       "std    7.165832e+04   10.482877     34096.960282    0.479864\n",
       "min    1.556669e+07   18.000000     15000.000000    0.000000\n",
       "25%    1.562676e+07   29.750000     43000.000000    0.000000\n",
       "50%    1.569434e+07   37.000000     70000.000000    0.000000\n",
       "75%    1.575036e+07   46.000000     88000.000000    1.000000\n",
       "max    1.581524e+07   60.000000    150000.000000    1.000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Dataset\n",
    "dataset = pd.read_csv('social_network_data.csv')\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.rename(columns={'User ID': 'id', 'Age': 'age', 'EstimatedSalary': 'est_salary', 'Purchased': 'purchased'},\n",
    "               inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset.iloc[:,[2,3]].values\n",
    "y = dataset.iloc[:,4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier1 = LogisticRegression()\n",
    "classifier1.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept: [-0.00510052]\n",
      "coefficient: [[-5.88775639e-03 -9.74009872e-09]]\n"
     ]
    }
   ],
   "source": [
    "print('intercept:', classifier1.intercept_)\n",
    "print('coefficient:', classifier1.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Training Set and Testing Set\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Scaling\n",
    "sc_X=StandardScaler()\n",
    "x_train=sc_X.fit_transform(x_train)\n",
    "x_test=sc_X.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the Logistic Model\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept: [-0.81452461]\n",
      "coefficient: [[2.11907143 1.17728586]]\n"
     ]
    }
   ],
   "source": [
    "print('intercept:', classifier.intercept_)\n",
    "print('coefficient:', classifier.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8** (Assignment): Perform gradient descent or Newton Raphson to find the Maximum Likelihood Estimates (MLE).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note_: I use all observations instead of splitting the dataset into training and test sets, since there is no clear need for doing so here.  \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{With } \\frac{\\delta\\mathcal{l}(\\beta)}{\\delta\\beta} = X^T(y-p) \\,\\text{ and } \\frac{\\delta^2\\mathcal{l}(\\beta)}{\\delta\\beta\\delta\\beta^T} = -X^TWX \\text{, we have}\\\\\n",
    "\\beta^{new} = \\beta^{old} + (X^TWX)^{-1}X^T(y-p)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(x, beta):\n",
    "    odds = np.exp(np.matmul(x, beta))\n",
    "    return (odds / (1 + odds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x, y, prob):\n",
    "    return np.matmul(x.transpose(), y - prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(x, prob):\n",
    "    return (-np.matmul(np.matmul(x.transpose(), np.diag(prob * (1 - prob))), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_beta(x, y, beta):\n",
    "    prob = probability(x, beta)\n",
    "    return (beta - np.matmul(np.linalg.inv(hessian(x, prob)), gradient(x, y, prob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_std = np.hstack((np.ones((x.shape[0], 1)), StandardScaler().fit_transform(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.13812197  2.44457954  1.22258176]\n"
     ]
    }
   ],
   "source": [
    "beta = np.array([0, 0, 0])\n",
    "tol = 1e-6\n",
    "beta_next = update_beta(x_std, y, beta)\n",
    "iters = 1\n",
    "while np.sum(abs(beta_next - beta)) > tol:\n",
    "    beta = beta_next\n",
    "    beta_next = update_beta(x_std, y, beta)\n",
    "    if iters % 100 == 0:\n",
    "        print('Iteration %d. MLE of beta:' % iters)\n",
    "        print(beta)\n",
    "beta = beta_next\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9**: What is the impact on the odds of a purchase with a dollar increase in EstimatedSalary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.000035901582615"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(beta[2] / np.std(x, axis=0)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beta[2] (1.222582) gives us the odds of a purchase with a sample standard deviation increase in estimated salary relative to the original odds. Divided by the sample standard deviation, it gives us the relative odds (of a purchase) with a dollar increase, which is 1.000036."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10**: What is the odds of purchase with an age of 38 and estimated salary of $60,000?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24479528277715504"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_std = (38 - means[0]) / stdevs[0]\n",
    "salary_std = (60000 - means[1]) / stdevs[1]\n",
    "np.exp(beta[0] + beta[1]*age_std + beta[2]*salary_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The odds of purchase are 0.244795, i.e. the individual is about 4 times as likely to not purchase as to purchase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics & Prediction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are unable to work with the residuals, as you would with OLS (we are using a binomial link function), a way to analyze classification accuracy and fit is through the _confusion matrix_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 11 (for my own curiosity)**: What are the performance metrics of the predictions obtained by fitting the model using the Newton-Raphson algorithm as above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the absence of further information, a reasonable criterion for predicting a positive outcome is when the predicted probability exceeds 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y, (probability(x_std, beta) > 0.5).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity = 0.713287, specificity = 0.851986\n"
     ]
    }
   ],
   "source": [
    "print('sensitivity = %f, specificity = %f' % (cm[1,1] / cm[1].sum(), cm[0,0] / (cm[0,0] + cm[1,0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has a sensitivity of {{ cm[0, 0] / cm[0].sum() }}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
